{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.6/site-packages (2.5.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (3.17.1)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /opt/conda/lib/python3.6/site-packages (from tensorflow) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.6/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.6/site-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (1.31.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (49.6.0.post20210108)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard~=2.5->tensorflow) (0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.4.1)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.6/site-packages (8.2.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.6/site-packages (0.17.2)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image) (8.2.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image) (2.5.1)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image) (3.3.4)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from scikit-image) (1.5.4)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.6/site-packages (from scikit-image) (2020.9.3)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image) (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.15.1 in /opt/conda/lib/python3.6/site-packages (from scikit-image) (1.19.5)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.6/site-packages (from scikit-image) (1.1.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.15.0)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image) (4.4.2)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Same as the first copy, this time I'm going to use Hailans data, and see if it makes a difference.\n",
    "\n",
    "%pip install tensorflow\n",
    "%pip install --upgrade Pillow\n",
    "%pip install -U scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "o2LiMYuq0f2V"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import numpy as np \n",
    "import random\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EgP2sIKD1NsC"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    super(Net, self).__init__()\n",
    "\n",
    "    input_width = 32\n",
    "    input_height = 32\n",
    "\n",
    "\n",
    "    self.net = nn.Sequential()\n",
    "\n",
    "\n",
    "    #Activation map of size: 3 * input_width * input_height\n",
    "    self.net.add_module('cv1', nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3, stride = 1, padding =  1, dilation = 1))\n",
    "\n",
    "    #Activation map of size: 64 * input_widht * input_height\n",
    "    self.net.add_module('r11', nn.ReLU())\n",
    "\n",
    "    #Activation map of size: 64 * input_width * input_height\n",
    "    self.net.add_module('cv2', nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 1, padding = 1, dilation = 1))\n",
    "\n",
    "    #Activation map of size 128* input_width * input_height\n",
    "    self.net.add_module('rl2', nn.ReLU())\n",
    "\n",
    "    #Activation map of size: 128 * input_width/2 * input_height/2\n",
    "    self.net.add_module('mp1', nn.MaxPool2d(kernel_size = 2, stride = None, padding = 0, dilation = 1))\n",
    "\n",
    "    #Activation map of size: 128 * input_width/2 * input_height/2\n",
    "    self.net.add_module('cv3', nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, stride = 1, padding = 1, dilation = 1))\n",
    "\n",
    "    #Activation map of size: 256 * input_width/2 * input_height/2\n",
    "    self.net.add_module('rl3', nn.ReLU())\n",
    "\n",
    "    #Activation map of size 256 * input_width/2 * input_height /2\n",
    "    self.net.add_module('cv4', nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 1, dilation = 1))\n",
    "\n",
    "    #Activation map of size 512 * input_width/2 * input_height/2\n",
    "    self.net.add_module('rl4', nn.ReLU())\n",
    "\n",
    "    #Activation map of size 512 * input_width/2 * input_height/2\n",
    "    self.net.add_module('mp2', nn.MaxPool2d(kernel_size = 2, stride = None, padding = 0, dilation = 1))\n",
    "\n",
    "    #Activation map of size 512 * input_width/4 * input_height/4\n",
    "    self.net.add_module('cv5', nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, stride = 1, padding = 1, dilation = 1))\n",
    "\n",
    "    #Activation map of size 512 * input_width/4 * input_height/4\n",
    "    self.net.add_module('rl5', nn.ReLU())\n",
    "\n",
    "    #activation map of size 512 * input_width/4 * input_height/4\n",
    "    #self.net.add_module('cv6', nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 1, padding = 1, dilation = 1))\n",
    "\n",
    "    #activation map of size 512 * input_width/4 * input_height/4\n",
    "    #self.net.add_module('rl6', nn.ReLU())\n",
    "\n",
    "\n",
    "\n",
    "    #Multilayer perceptron\n",
    "    self.net.add_module('dp1', nn.Dropout2d(p = 0.25))\n",
    "    self.net.add_module('fl1', nn.Flatten())\n",
    "    self.net.add_module('fc1', nn.Linear(in_features = 2768896, out_features = 128))\n",
    "    self.net.add_module('rl7', nn.ReLU())\n",
    "    self.net.add_module('fc2', nn.Linear(in_features = 128, out_features = 4))\n",
    "    self.net.add_module('sm1', nn.LogSoftmax(dim = 1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.net(x.float())\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.4)\n",
    "\n",
    "# Let's define a Loss function\n",
    "\n",
    "lossfun = nn.CrossEntropyLoss()  # Use nn.CrossEntropyLoss with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1801"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "imgs = datasets.ImageFolder('Data/Padded', transform = transforms.ToTensor())\n",
    "\n",
    "f = open(\"Data/Target/y_target.txt\")\n",
    "\n",
    "path = \"data2/train\"\n",
    "\n",
    "train_path = os.path.join(path, 'images', '*')\n",
    "train_paths = sorted(glob.glob(train_path)) \n",
    "\n",
    "path = \"data2/test\"\n",
    "test_path = os.path.join(path, 'images', '*')\n",
    "test_paths = sorted(glob.glob(test_path))\n",
    "\n",
    "path = \"data2/val\"\n",
    "val_path = os.path.join(path, 'images', '*')\n",
    "val_paths = sorted(glob.glob(val_path))\n",
    "\n",
    "len(train_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416, 416, 3)\n",
      "(416, 416, 3)\n",
      "(416, 416, 3)\n",
      "(416, 416, 3)\n",
      "(416, 416, 3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    img = io.imread(train_paths[i])\n",
    "    print(img.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting to make a custom dataset, so that we can use the below training algorthm. \n",
    "\n",
    "class TrashDataset():\n",
    "    def __init__(self, target_vector_file, train, transform = None):\n",
    "        dataset = {}\n",
    "        \n",
    "        self.train = train\n",
    "        \n",
    "        if train:\n",
    "            target_file = open(\"data2/Target/y_train_with_name.txt\", \"r\")\n",
    "        else:\n",
    "            target_file = open(\"data2/Target/y_test_with_name.txt\", \"r\")\n",
    "        img_name = target_file.readline()\n",
    "        i = 0\n",
    "        classifications = []\n",
    "        img_names = []\n",
    "        while(img_name != \"\"):\n",
    "            img_names.append(str(img_name).strip())\n",
    "            classifications.append(str(target_file.readline()).strip())\n",
    "            img_name = target_file.readline()\n",
    "        target_file.close()\n",
    "        \n",
    "        dataset = []\n",
    "        \n",
    "        for i in range(len(img_names)):\n",
    "            dataset.append( (img_names[i], classifications[i]))\n",
    "        \n",
    "        self.dataframe = pd.DataFrame(dataset)\n",
    "        #self.dataframe = self.dataframe.sample(frac = 1, random_state = 42)\n",
    "        \n",
    "        print(self.dataframe)\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__ (self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            idx = idx.tolist(index)\n",
    "        img_name = self.dataframe.iloc[index, 0]\n",
    "        if self.train:\n",
    "            img = io.imread(\"data2/train/images/\" + img_name[:-5] + \".jpg\")\n",
    "            #print(img_name[:-5], \".jpg\")\n",
    "        else:\n",
    "            img = io.imread(\"data2/test/images/\" + img_name[:-5] + \".jpg\")\n",
    "        classification = self.dataframe.iloc[index, 1]\n",
    "        sample = {\"image\": img, \"classification\": classification}\n",
    "        sample = self.transform(sample)\n",
    "        return sample\n",
    "        \n",
    "\n",
    "        \n",
    "class ToTensor(object):\n",
    "    #Class to turn nparrays to tensor\n",
    "    def __call__(self, sample):\n",
    "        image, classification= sample[\"image\"], sample[\"classification\"]\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {\"image\": torch.from_numpy(image), \"classification\": int(classification)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      0  1\n",
      "0     003a1a43-509b-4ce7-922b-2a6851e4ef41_1296e748-...  3\n",
      "1     003a1a43-509b-4ce7-922b-2a6851e4ef41_1296e748-...  3\n",
      "2     003a1a43-509b-4ce7-922b-2a6851e4ef41_1296e748-...  3\n",
      "3     003d0583-7137-402d-82a4-de182b2287af_7e6dee91-...  1\n",
      "4     003d0583-7137-402d-82a4-de182b2287af_7e6dee91-...  1\n",
      "...                                                 ... ..\n",
      "1796  ffec0471-dd09-4e0c-b34a-b66f8fdb700d_4c82e9b6-...  0\n",
      "1797  fff9a9bc-350a-454c-a9e5-254d388786b6_b7520d69-...  2\n",
      "1798  fff9a9bc-350a-454c-a9e5-254d388786b6_b7520d69-...  2\n",
      "1799  fff9a9bc-350a-454c-a9e5-254d388786b6_e2481186-...  2\n",
      "1800  fff9a9bc-350a-454c-a9e5-254d388786b6_e2481186-...  2\n",
      "\n",
      "[1801 rows x 2 columns]\n",
      "                                                    0  1\n",
      "0   03d2b09f-4b13-4259-af6d-74ca47e1b6f5_1d0c9370-...  2\n",
      "1   0be28175-fb34-4aa9-9b28-c34754eed48a_dcafe8ad-...  0\n",
      "2   0c826e4d-3b5f-41da-8d0c-acbf92971e63_a7b674ba-...  1\n",
      "3   1bd3fd7c-e94a-48b7-bedc-89b918d26f4c_42de588b-...  3\n",
      "4   1c89b3c2-3ebf-4ff1-b5c7-0e7605caf491_169b36ca-...  0\n",
      "..                                                ... ..\n",
      "93  ffd8a0b0-c4f9-4797-8252-9a1d9e4ad09b_19a19569-...  1\n",
      "94  ffe49e8e-dfd0-4729-86ee-13e8fcc0c54d_0eb99cb9-...  3\n",
      "95  ffe795ec-a476-4648-a578-14b179940748_91c1f773-...  3\n",
      "96  ffeb6b81-d2c7-4d50-9b65-0f5299b0bccf_54881181-...  1\n",
      "97  fff1c628-4b3b-42e1-8075-d5a9bfdb75f0_da73e6e9-...  1\n",
      "\n",
      "[98 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TrashDataset(\"Data/Target/y_target_with_name.txt\", train = True, transform = ToTensor())\n",
    "test_dataset = TrashDataset(\"Data/Target/y_target_with_name.txt\", train = False, transform = ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-pJZfx8PIKd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tAverage Loss: 0.389072\tAverage Accuracy: 0.86\n",
      "starting to test...\n",
      "Average Loss: 2.010589\tAverage Accuracy: 0.25\n",
      "Train Epoch: 2\tAverage Loss: 0.240675\tAverage Accuracy: 0.93\n",
      "starting to test...\n",
      "Average Loss: 2.026467\tAverage Accuracy: 0.58\n",
      "Train Epoch: 3\tAverage Loss: 0.052259\tAverage Accuracy: 0.99\n",
      "starting to test...\n",
      "Average Loss: 2.679913\tAverage Accuracy: 0.55\n",
      "Train Epoch: 4\tAverage Loss: 0.124101\tAverage Accuracy: 0.97\n",
      "starting to test...\n",
      "Average Loss: 2.075985\tAverage Accuracy: 0.58\n",
      "Train Epoch: 5\tAverage Loss: 0.018768\tAverage Accuracy: 1.00\n",
      "starting to test...\n",
      "Average Loss: 2.263943\tAverage Accuracy: 0.57\n",
      "Train Epoch: 6\tAverage Loss: 0.008693\tAverage Accuracy: 1.00\n",
      "starting to test...\n",
      "Average Loss: 2.663777\tAverage Accuracy: 0.57\n",
      "Train Epoch: 7\tAverage Loss: 0.005492\tAverage Accuracy: 1.00\n",
      "starting to test...\n",
      "Average Loss: 2.762723\tAverage Accuracy: 0.58\n",
      "Train Epoch: 8\tAverage Loss: 0.003638\tAverage Accuracy: 1.00\n",
      "starting to test...\n",
      "Average Loss: 2.892420\tAverage Accuracy: 0.57\n",
      "Train Epoch: 9\tAverage Loss: 0.154961\tAverage Accuracy: 0.97\n",
      "starting to test...\n",
      "Average Loss: 2.256324\tAverage Accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "#Training the model, (here we go!)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True, num_workers = 0)\n",
    "\n",
    "def train(model, train_loader, epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Define train epochs\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = False)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "\n",
    "        # iterate through train dataset\n",
    "        epoch_loss = []\n",
    "        epoch_accu = []\n",
    "\n",
    "        for batch_idx, sample in enumerate(train_loader):\n",
    "            \n",
    "            #data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            data = sample[\"image\"]\n",
    "            target = sample[\"classification\"]\n",
    "\n",
    "            #print(data.shape)\n",
    "\n",
    "            # get output\n",
    "            output = model(data)\n",
    "\n",
    "            # compute loss function\n",
    "            loss = lossfun(output, target)\n",
    "            \n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # run optimizer \n",
    "            optimizer.step()\n",
    "\n",
    "            # bookkeeping\n",
    "            accuracy = (output.argmax(-1) == target).float().mean()\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_accu.append(accuracy.item())\n",
    "\n",
    "            #if batch_idx % 12 == 0 and batch_idx == 0:\n",
    "                #print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {:.2f}'.format(\n",
    "                    #epoch+1, batch_idx*32, len(train_loader.dataset),\n",
    "                    #100. * batch_idx / len(train_loader), sum(epoch_loss)/len(epoch_loss), sum(epoch_accu)/len(epoch_accu))\n",
    "        \n",
    "        print('Train Epoch: {}\\tAverage Loss: {:.6f}\\tAverage Accuracy: {:.2f}'.format(\n",
    "            epoch+1, sum(epoch_loss)/len(epoch_loss), sum(epoch_accu)/len(epoch_accu)))\n",
    "        \n",
    "        test(test_loader)\n",
    "            \n",
    "    # save network\n",
    "    \n",
    "    return epoch_loss, epoch_accu\n",
    "\n",
    "epoch_loss, epoch_accu = train(model, train_loader, epochs=26)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader):\n",
    "    \n",
    "    total_loss = []\n",
    "    total_accu = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"starting to test...\")\n",
    "    \n",
    "    for batch_idx, sample in enumerate(test_loader):\n",
    "            \n",
    "\n",
    "            #data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            data = sample[\"image\"]\n",
    "            target = sample[\"classification\"]\n",
    "\n",
    "            #print(data.shape)\n",
    "\n",
    "            # get output\n",
    "            output = model(data)\n",
    "\n",
    "            # compute loss function\n",
    "            loss = lossfun(output, target)\n",
    "\n",
    "            # bookkeeping\n",
    "            accuracy = (output.argmax(-1) == target).float().mean()\n",
    "            total_loss.append(loss.item())\n",
    "            total_accu.append(accuracy.item())\n",
    "            \n",
    "\n",
    "    print('Average Loss: {:.6f}\\tAverage Accuracy: {:.2f}'.format(\n",
    "            sum(total_loss)/len(total_loss), sum(total_accu)/len(total_accu)))\n",
    "            \n",
    "    \n",
    "    return total_loss, total_accu\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = False)\n",
    "\n",
    "#test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train attempt 2\n",
    "\n",
    "#Set up permissions and enviroment variables. \n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role\n",
    "print(role)\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = \"e-team6\"\n",
    "prefix = \"ic-fulltraining\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, \"image-classification\", repo_version = \"latest\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preperation\n",
    "\n",
    "import boto3 \n",
    "\n",
    "#Function to transfer data to s3 for training. \n",
    "\n",
    "def upload_to_s3(channel, file):\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    data = open(file, \"rb\")\n",
    "    key = channel + \"/\" + file\n",
    "    s3.Bucket(bucket).put_object(Key = key, Body = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into to channels -- train and test\n",
    "\n",
    "s3train = \"s3://{}/{}/train\".format(bucket, prefix)\n",
    "s3validation = \"s3://{}/{}/validation\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload images into validation and training channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_max_run has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_volume_size has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "s3_output_location = \"s3://{}/{}/output\".format(bucket, prefix)\n",
    "\n",
    "ic = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role, \n",
    "    train_instance_count = 1, \n",
    "    train_instance_type = \"m1.p2.xlarge\",\n",
    "    train_volume_size = 50, \n",
    "    train_max_run = 360000,\n",
    "    input_model = \"file\",\n",
    "    output_path = s3_output_location, \n",
    "    sagemaker_session = sess\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up hyperparmeters in the algorithm.\n",
    "\n",
    "ic.set_hyperparameters(\n",
    "    num_layers = 18, \n",
    "    image_shape = \"3, 5312, 4032\",\n",
    "    num_classes = 4, \n",
    "    num_training_examples = 400,\n",
    "    mini_batch_size = 32,\n",
    "    epochs = 10,\n",
    "    learning_rate = 0.1,\n",
    "    top_k = 2#I have no idea what this parameter does\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3train, \n",
    "    distribution = \"FullyReplicated\",\n",
    "    content_type = \"image/jpeg\",\n",
    "    s3_data_type = \"S3Prefix\",\n",
    "    )\n",
    "\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    s3validation, \n",
    "    distribution = \"FullyRepliatied\",\n",
    "    content_type = \"image/jpeg\", \n",
    "    s3_data_type = \"S3Prefix\"\n",
    "    )\n",
    "\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "PATH = \"train\"\n",
    "image_path = os.path.join(PATH, 'images', '*')\n",
    "image_paths = sorted(glob.glob(image_path)) \n",
    "\n",
    "for path in image_paths:\n",
    "    upload_to_s3(\"train\", path)\n",
    "    \n",
    "PATH = \"valid\"    \n",
    "image_path = os.path.join(PATH, \"images\", \"*\")\n",
    "imagePaths = sorted(glob.glob(image_path))\n",
    "\n",
    "for path in image_paths:\n",
    "    upload_to_s3(\"validation\", path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'function' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-819888737956>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1421\u001b[0m             \u001b[0mall\u001b[0m \u001b[0minformation\u001b[0m \u001b[0mabout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstarted\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m         \"\"\"\n\u001b[0;32m-> 1423\u001b[0;31m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1424\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36m_get_train_args\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1453\u001b[0m                 )\n\u001b[1;32m   1454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m         \u001b[0mcurrent_hyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sagemaker/job.py\u001b[0m in \u001b[0;36m_load_config\u001b[0;34m(inputs, estimator, expand_role, validate_uri)\u001b[0m\n\u001b[1;32m     68\u001b[0m         role = (\n\u001b[1;32m     69\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mexpand_role\u001b[0;34m(self, role)\u001b[0m\n\u001b[1;32m   3468\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mAWS\u001b[0m \u001b[0mIAM\u001b[0m \u001b[0mrole\u001b[0m \u001b[0mARN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3469\u001b[0m         \"\"\"\n\u001b[0;32m-> 3470\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3471\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3472\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'function' is not iterable"
     ]
    }
   ],
   "source": [
    "ic.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CNN.ipynb",
   "provenance": []
  },
  "instance_type": "ml.m5.12xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
